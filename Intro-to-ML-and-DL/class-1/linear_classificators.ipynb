{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Generation \n",
    "Let us generate a 2 dimensional dataset for $n$ samples that arise from normally distributed data, such that two _classes_ $\\mathcal{D}_{1,2}$ arise, for means $\\mu_{1}=\\left(1,1\\right)^{\\intercal}$, $\\mu_{1}=\\left(-1,-1\\right)^{\\intercal}$ and  variance $\\bm{\\sigma}= 2\\mathbb{I}_{2\\times 2}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_gen(samples=20):\n",
    "    mu_1 = np.array([4, 4])\n",
    "    mu_2 = np.array([-4,-4])\n",
    "    sigma = 5* np.eye(2,2)\n",
    "    class1 = np.random.multivariate_normal(mu_1, sigma, samples)    \n",
    "    class2 = np.random.multivariate_normal(mu_2, sigma, samples)\n",
    "    return class1, class2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustration sake let us plot the class scatter distribution, recall that $\\mathcal{C}_i \\in \\Reals^{n \\times 2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/jpacheco/git/my_courses/Intro-to-ML-and-DL/class-1/linear_classificators.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jpacheco/git/my_courses/Intro-to-ML-and-DL/class-1/linear_classificators.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m class_samples \u001b[39m=\u001b[39m \u001b[39m20000\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jpacheco/git/my_courses/Intro-to-ML-and-DL/class-1/linear_classificators.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m class1, class2 \u001b[39m=\u001b[39m class_gen(class_samples)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jpacheco/git/my_courses/Intro-to-ML-and-DL/class-1/linear_classificators.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m fig, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jpacheco/git/my_courses/Intro-to-ML-and-DL/class-1/linear_classificators.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m ax\u001b[39m.\u001b[39mplot(class1[:,\u001b[39m0\u001b[39m], class1[:,\u001b[39m1\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mok\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/home/jpacheco/git/my_courses/Intro-to-ML-and-DL/class-1/linear_classificators.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jpacheco/git/my_courses/Intro-to-ML-and-DL/class-1/linear_classificators.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclass_gen\u001b[39m(samples\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jpacheco/git/my_courses/Intro-to-ML-and-DL/class-1/linear_classificators.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     mu_1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m4\u001b[39m, \u001b[39m4\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jpacheco/git/my_courses/Intro-to-ML-and-DL/class-1/linear_classificators.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     mu_2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m-\u001b[39m\u001b[39m4\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m4\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jpacheco/git/my_courses/Intro-to-ML-and-DL/class-1/linear_classificators.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     sigma \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39meye(\u001b[39m2\u001b[39m,\u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "class_samples = 20000\n",
    "class1, class2 = class_gen(class_samples)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(class1[:,0], class1[:,1], 'ok')\n",
    "ax.plot(class2[:,0], class2[:,1], 'xr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the data scatter suggests that the classes are linearly spearable.\n",
    "In order to perform supervised learning classification, let us build the array of tags for each class $Y_1, \\ Y_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1 = np.ones((class_samples,1));\n",
    "Y2 = -1 * Y1; \n",
    "Y_label = np.vstack([Y1,Y2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40000, 1), (40000, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.vstack([class1,class2]);\n",
    "extra_dimension = np.ones((2*class_samples,1));\n",
    "(extra_dimension.shape, X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40000, 2), (40000, 3))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tilde = np.hstack([extra_dimension,X])\n",
    "X.shape, X_tilde.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our extended data, we've only require to obtain the so-called _covariant matrix_ $C = \\tilde{\\bm{X}}^\\intercal \\tilde{\\bm{X}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CovX = np.matmul(X_tilde.T, X_tilde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights arise as $C^{-1}\\tilde{X}^{\\intercal}\\tilde{Y}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00130542],\n",
       "       [0.12205399],\n",
       "       [0.12027562]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.matmul( np.linalg.inv(CovX) , np.matmul(X_tilde.T,Y_label) )\n",
    "W "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimated_value = np.matmul(X_tilde, W)\n",
    "estimated_value.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the linear classificator is an scalar function of the form $\\bm{h}\\left(\\bm{x}^{(i)}; \\bm{\\Theta} \\right)\\equiv\\hat{\\bm{y}}^{(i)} = \\bm{W}^\\intercal \\bm{x}^{(i)} + \\bm{b} $, however, for this binary class prediction, we still need to assign a label for each hypotheses. From the $\\verb+Y_labels+$ variable, we know that each sample $\\bm{x}^{(i)}$ is assign to an binary label $\\bm{y}^{(i)} \\in \\left\\lbrace 1, -1 \\right\\rbrace:=\\mathcal{\\sigma}_{\\text{b}}$, hence our prediction arise by simply taking the sign of the $\\verb+estimated_value+$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels  = np.sign(estimated_value)\n",
    "predicted_labels[predicted_labels==0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can evaluate the prediction performance by appealing to the 2-norm, or LSQ:\n",
    "\\begin{gather*}\n",
    "\\mathcal{L} = \\sum_{i = 1}^{n}\\left(\\hat{\\bm{y}}^{(i)} - \\bm{y}^{(i)}\\right)^2 \\\\\n",
    "            = \\sum_{i = 1}^{n}\\left\\|\\hat{\\bm{y}}^{(i)} - \\bm{y}^{(i)}\\right\\|^2 \\\\\n",
    "            = \\sum_{i = 1}^{n}\\left\\langle\\hat{\\bm{y}}^{(i)} - \\bm{y}^{(i)},\\hat{\\bm{y}}^{(i)} - \\bm{y}^{(i)}\\right\\rangle \\\\\n",
    "\\mathcal{L} = \\sum_{i = 1}^{n}\\left(\\hat{\\bm{y}}^{(i)} - \\bm{y}^{(i)}\\right)^{\\intercal} \\left(\\hat{\\bm{y}}^{(i)} - \\bm{y}^{(i)}\\right)\n",
    "\\end{gather*}\n",
    "Notice from the shape of our predictions that we can lump all samples and perform the metric evaluation within a oneliner;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_error = predicted_labels- Y_label\n",
    "prediction_error.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squared_performance_error = np.dot(prediction_error.T,prediction_error)\n",
    "squared_performance_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And _voil√†_ we've found that the linearly separable data has been perfectly predicted by the linear classificator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, for this codding example we've ommited the _Supervised Training Process_, as we've used the entire dataset for training and validation. However, we've arrived to valuable insights about the _magic_ behind the simpler supervised learning algorithm. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro_MDL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
